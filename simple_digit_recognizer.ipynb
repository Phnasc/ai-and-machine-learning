{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3o1+q2N3pxSzgA1JLD1lI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phnasc/ai-and-machine-learning/blob/main/simple_digit_recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Digit Recognition Neural Network\n",
        "This project demonstrates a simple neural network implementation using numpy to recognize hand-written digits from the MNIST dataset. The code consists of various components to load, preprocess, train, and evaluate the model."
      ],
      "metadata": {
        "id": "-nSJ-C5ZViVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The provided code encapsulates a neural network model within the\n",
        "NeuralNetwork class. It defines hyperparameters like hidden_layer_size, learning_rate, and iterations, initializing model parameters. The forward propagation computes layer activations with ReLU and softmax functions. Backpropagation computes gradients for parameter updates using chain rule, and the training loop executes gradient descent to optimize parameters. The model's accuracy is monitored during training"
      ],
      "metadata": {
        "id": "d26DEI3fdnO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Foward Propagation:**\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Z}^{[1]} &= \\mathbf{W}^{[1]}\\mathbf{X} + \\mathbf{b}^{[1]} \\\\\n",
        "\\mathbf{A}^{[1]} &= g_{\\text{ReLU}}(\\mathbf{Z}^{[1]}) \\\\\n",
        "\\mathbf{Z}^{[2]} &= \\mathbf{W}^{[2]}\\mathbf{A}^{[1]} + \\mathbf{b}^{[2]} \\\\\n",
        "\\mathbf{A}^{[2]} &= g_{\\text{softmax}}(\\mathbf{Z}^{[2]})\n",
        "\\end{align}\n",
        "\n",
        "* **Backward Propagation:**\n",
        "\n",
        "\\begin{align*}\n",
        "dZ^{[2]} &= A^{[2]} - Y \\\\\n",
        "dW^{[2]} &= \\frac{1}{m} dZ^{[2]} A^{[1]T} \\\\\n",
        "dB^{[2]} &= \\frac{1}{m} \\sum dZ^{[2]} \\\\\n",
        "dZ^{[1]} &= W^{[2]T} dZ^{[2]} \\odot g[1]'(Z^{[1]}) \\\\\n",
        "dW^{[1]} &= \\frac{1}{m} dZ^{[1]} A^{[0]T} \\\\\n",
        "dB^{[1]} &= \\frac{1}{m} \\sum dZ^{[1]} \\\\\n",
        "\\end{align*}\n",
        "\\\n",
        "\n",
        "* **Parameter updates**\n",
        "\n",
        "\\begin{align*}\n",
        "W^{[2]} &:= W^{[2]} - \\alpha dW^{[2]} \\\\\n",
        "b^{[2]} &:= b^{[2]} - \\alpha db^{[2]} \\\\\n",
        "W^{[1]} &:= W^{[1]} - \\alpha dW^{[1]} \\\\\n",
        "b^{[1]} &:= b^{[1]} - \\alpha db^{[1]} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "where *alpha* is the learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "a1LPN7fMeYL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Defining a class for the Neural Network\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, hidden_layer_size, learning_rate, iterations):\n",
        "        # Initializing hyperparameters\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        # Loading and preprocessing data\n",
        "        data = pd.read_csv(file_path)\n",
        "        data = np.array(data)\n",
        "        np.random.shuffle(data)\n",
        "\n",
        "        m, n = data.shape\n",
        "        # Splitting data into development and training sets\n",
        "        data_dev = data[:1000].T\n",
        "        self.Y_dev = data_dev[0]\n",
        "        self.X_dev = data_dev[1:n] / 255.\n",
        "\n",
        "        data_train = data[1000:m].T\n",
        "        self.Y_train = data_train[0]\n",
        "        self.X_train = data_train[1:n] / 255.\n",
        "\n",
        "    def init_params(self):\n",
        "        # Initializing model parameters\n",
        "        self.W1 = np.random.rand(self.hidden_layer_size, 784) - 0.5\n",
        "        self.b1 = np.random.rand(self.hidden_layer_size, 1) - 0.5\n",
        "        self.W2 = np.random.rand(10, self.hidden_layer_size) - 0.5\n",
        "        self.b2 = np.random.rand(10, 1) - 0.5\n",
        "\n",
        "    # ReLU activation function\n",
        "    def ReLU(self, Z):\n",
        "        return np.maximum(Z, 0)\n",
        "\n",
        "    # Softmax activation function\n",
        "    def softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z))  # Avoiding numerical instability\n",
        "        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward_prop(self, X):\n",
        "        Z1 = self.W1.dot(X) + self.b1\n",
        "        A1 = self.ReLU(Z1)\n",
        "        Z2 = self.W2.dot(A1) + self.b2\n",
        "        A2 = self.softmax(Z2)\n",
        "        return Z1, A1, Z2, A2\n",
        "\n",
        "    # ReLU derivative\n",
        "    def ReLU_deriv(self, Z):\n",
        "        return Z > 0\n",
        "\n",
        "    # Converting labels to one-hot encoded format\n",
        "    def one_hot(self, Y):\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "        return one_hot_Y.T\n",
        "\n",
        "    # Backpropagation\n",
        "    def backward_prop(self, Z1, A1, Z2, A2, X, Y):\n",
        "        one_hot_Y = self.one_hot(Y)\n",
        "        dZ2 = A2 - one_hot_Y\n",
        "        dW2 = 1 / self.X_train.shape[1] * dZ2.dot(A1.T)\n",
        "        db2 = 1 / self.X_train.shape[1] * np.sum(dZ2, axis=1, keepdims=True)\n",
        "        dZ1 = self.W2.T.dot(dZ2) * self.ReLU_deriv(Z1)\n",
        "        dW1 = 1 / self.X_train.shape[1] * dZ1.dot(X.T)\n",
        "        db1 = 1 / self.X_train.shape[1] * np.sum(dZ1, axis=1, keepdims=True)\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    # Parameter update\n",
        "    def update_params(self, dW1, db1, dW2, db2):\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "\n",
        "    # Getting predictions from output layer\n",
        "    def get_predictions(self, A2):\n",
        "        return np.argmax(A2, axis=0)\n",
        "\n",
        "    # Calculating accuracy\n",
        "    def get_accuracy(self, predictions, Y):\n",
        "        return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "    # Gradient Descent training loop\n",
        "    def gradient_descent(self):\n",
        "        self.init_params()\n",
        "        for i in range(self.iterations):\n",
        "            Z1, A1, Z2, A2 = self.forward_prop(self.X_train)\n",
        "            dW1, db1, dW2, db2 = self.backward_prop(Z1, A1, Z2, A2, self.X_train, self.Y_train)\n",
        "            self.update_params(dW1, db1, dW2, db2)\n",
        "            if i % 10 == 0:\n",
        "                predictions = self.get_predictions(A2)\n",
        "                accuracy = self.get_accuracy(predictions, self.Y_train)\n",
        "                print(f\"Iteration: {i}, Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    # Making predictions\n",
        "    def make_predictions(self, X):\n",
        "        _, _, _, A2 = self.forward_prop(X)\n",
        "        predictions = self.get_predictions(A2)\n",
        "        return predictions\n",
        "\n",
        "    # Testing a single prediction\n",
        "    def test_prediction(self, index):\n",
        "        current_image = self.X_train[:, index, None]\n",
        "        prediction = self.make_predictions(self.X_train[:, index, None])\n",
        "        label = self.Y_train[index]\n",
        "\n",
        "        print(\"Prediction:\", prediction)\n",
        "        print(\"Label:\", label)\n",
        "\n",
        "        current_image = current_image.reshape((28, 28)) * 255\n",
        "        plt.gray()\n",
        "        plt.imshow(current_image, interpolation='nearest')\n",
        "        plt.show()\n",
        "\n",
        "    # Evaluating development set accuracy\n",
        "    def evaluate_dev_set(self):\n",
        "        dev_predictions = self.make_predictions(self.X_dev)\n",
        "        accuracy = self.get_accuracy(dev_predictions, self.Y_dev)\n",
        "        print(f\"Development Set Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Creating an instance of the NeuralNetwork class\n",
        "    neural_net = NeuralNetwork(hidden_layer_size=128, learning_rate=0.1, iterations=500)\n",
        "\n",
        "    # Loading data\n",
        "    neural_net.load_data('/content/train.csv')\n",
        "\n",
        "    # Training the neural network\n",
        "    neural_net.gradient_descent()\n",
        "\n",
        "    # Evaluating on the development set\n",
        "    neural_net.evaluate_dev_set()\n",
        "\n",
        "    # Testing a single prediction\n",
        "    neural_net.test_prediction(index=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JRWubVhiMfQm",
        "outputId": "0a6cce67-81dc-42b9-c608-e2d81872c50c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0, Accuracy: 7.29%\n",
            "Iteration: 10, Accuracy: 52.18%\n",
            "Iteration: 20, Accuracy: 64.97%\n",
            "Iteration: 30, Accuracy: 71.13%\n",
            "Iteration: 40, Accuracy: 74.63%\n",
            "Iteration: 50, Accuracy: 77.14%\n",
            "Iteration: 60, Accuracy: 78.89%\n",
            "Iteration: 70, Accuracy: 80.29%\n",
            "Iteration: 80, Accuracy: 81.38%\n",
            "Iteration: 90, Accuracy: 82.25%\n",
            "Iteration: 100, Accuracy: 82.95%\n",
            "Iteration: 110, Accuracy: 83.59%\n",
            "Iteration: 120, Accuracy: 84.13%\n",
            "Iteration: 130, Accuracy: 84.64%\n",
            "Iteration: 140, Accuracy: 85.09%\n",
            "Iteration: 150, Accuracy: 85.50%\n",
            "Iteration: 160, Accuracy: 85.89%\n",
            "Iteration: 170, Accuracy: 86.16%\n",
            "Iteration: 180, Accuracy: 86.45%\n",
            "Iteration: 190, Accuracy: 86.71%\n",
            "Iteration: 200, Accuracy: 87.01%\n",
            "Iteration: 210, Accuracy: 87.27%\n",
            "Iteration: 220, Accuracy: 87.53%\n",
            "Iteration: 230, Accuracy: 87.74%\n",
            "Iteration: 240, Accuracy: 87.94%\n",
            "Iteration: 250, Accuracy: 88.14%\n",
            "Iteration: 260, Accuracy: 88.31%\n",
            "Iteration: 270, Accuracy: 88.47%\n",
            "Iteration: 280, Accuracy: 88.60%\n",
            "Iteration: 290, Accuracy: 88.75%\n",
            "Iteration: 300, Accuracy: 88.90%\n",
            "Iteration: 310, Accuracy: 89.04%\n",
            "Iteration: 320, Accuracy: 89.13%\n",
            "Iteration: 330, Accuracy: 89.26%\n",
            "Iteration: 340, Accuracy: 89.36%\n",
            "Iteration: 350, Accuracy: 89.49%\n",
            "Iteration: 360, Accuracy: 89.60%\n",
            "Iteration: 370, Accuracy: 89.71%\n",
            "Iteration: 380, Accuracy: 89.83%\n",
            "Iteration: 390, Accuracy: 89.92%\n",
            "Iteration: 400, Accuracy: 90.00%\n",
            "Iteration: 410, Accuracy: 90.09%\n",
            "Iteration: 420, Accuracy: 90.19%\n",
            "Iteration: 430, Accuracy: 90.26%\n",
            "Iteration: 440, Accuracy: 90.34%\n",
            "Iteration: 450, Accuracy: 90.43%\n",
            "Iteration: 460, Accuracy: 90.51%\n",
            "Iteration: 470, Accuracy: 90.57%\n",
            "Iteration: 480, Accuracy: 90.68%\n",
            "Iteration: 490, Accuracy: 90.75%\n",
            "Development Set Accuracy: 90.90%\n",
            "Prediction: [7]\n",
            "Label: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Accuracy Progression:** The displayed accuracy percentages illustrate the improvement in the model's performance as training iterations advance. At the beginning, the accuracy is around **7.29%**, which is essentially random guessing since there are 10 classes (digits 0 to 9). However, as the iterations increase, the accuracy rises steadily, reaching an accuracy of 90.75% on the training set by the last iteration.\n",
        "\n",
        "2. **Development Set Accuracy:** The accuracy achieved on the development set is a notable **90.90%**, which aligns closely with the accuracy we got on the training set. This indicates that the model is performing consistently and has learned patterns in the data rather than merely memorizing the training examples."
      ],
      "metadata": {
        "id": "PxcFZk7Mcmt-"
      }
    }
  ]
}